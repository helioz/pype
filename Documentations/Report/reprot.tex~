\documentclass[a4paper,11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
% \usepackage{amsmath}
% \usepackage{amssymb,amsfonts,textcomp}
% \usepackage{color}
% \usepackage{array}
% \usepackage{supertabular}
% \usepackage{hhline}
\usepackage{hyperref}
\usepackage{cite}
% \usepackage{etoolbox}
\usepackage{acro}
\usepackage{graphicx}
\usepackage[nodayofweek]{datetime}

\include{helpers/acronyms}
\DeclareGraphicsRule{*}{mps}{*}{}
\newcommand{\mparagraph}[1]{\paragraph{#1}\mbox{}\\}

% Commands
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines

\begin{document}

	\begin{titlepage}
		\begin{centering}
		 
		%	HEADING SECTIONS
		\textbf{\textit{\large{B.Tech Project Report}}}\\[0.5cm]
		
		\textsc{\textbf{\LARGE{Decentralised Video Messenger
over a peer-2-peer Network }}}\\[1.5cm]

		\large{Submitted in partial fulfilment for the award of the Degree of Bachelor of Technology in Computer Science and Engineering}\\[1.5cm]

		\large{Submitted by}\\[0.5cm]

		\textbf{Raeesul Asad     (Roll No 13400030)}\\
		\textbf{Shamnas T. V.     (Roll No 13400032)}\\
                \textbf{Soorej Jones Pothoor     (Roll No 13 400 058)}\\
		\textbf{Jerry Raju     (Roll No 13400038)}\\[1.5cm]
		
		{Under the guidance of}\\[0.25cm]
		\large{Dr. Salim A.}\\[0.5cm]

		\includegraphics[width=5cm]{images/logo.jpg} 

		Department of Computer Science and Engineering\\
		\textsc{College of Engineering, Trivandrum}\\
		\textsc{Kerala}\\
		\textsc{May 2017}\\
		\vfill % Fill the rest of the page with whitespace
		\end{centering}
	\end{titlepage}

	\begin{titlepage}
		\begin{centering}
			\textbf{\textit{\LARGE\textsc{{certificate}}}}\\[0.5cm]
			\includegraphics[width=5cm]{images/logo.jpg}\\

		\end{centering}

		\large{This is to certify that the thesis entitled ``Decentralised Video Messenger over a peer-2-peer Network'' is a bonafide record of the major project done by \textbf{Raeesul Asad} (Roll No 13400030), \textbf{Shamnsas T. V.} \textbf{Soorej Jones Pothoor} (Roll No 13 400 058) and \textbf{Jerry Raju} (Roll No 13400038) under my supervision and guidance, in partial fulfilment for the award of the Degree of Bachelor of Technology in Computer Science and Engineering from the University of Kerala for the year 2017.}\\[1.5cm]

		\begin{minipage}{0.4\textwidth}
		\begin{flushleft}
		\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.6\textwidth}
		\begin{centering} \large
		\large{Dr. Salim A.}\\
		\small{(Guide)}\\
		\small{\textit{\textbf{Associate Professor}}}\\
		\small{\textit{\textbf{Dept. of Computer Science and Engineering}}}\\[1.5cm]

		\large{Mrs. Liji P. I.}\\
		\small{\textit{\textbf{Professor and Head}}}\\
		\small{\textit{\textbf{Dept. of Computer Science and Engineering}}}\\
		\end{centering}
		\end{minipage}\\[1.0cm]

		\begin{flushleft}
		Place: Trivandrum\\
		Date:  11-05-2017\\
		\end{flushleft}
		\vfill % Fill the rest of the page with whitespace
	\end{titlepage}

	% TODO Ack before this
	\pagenumbering{roman}
	
	\begin{abstract}
In an age where communication is of paramount importance, it is considerably worrying that such networks are controlled by private companies. New information shows proof of an unprecedented amount of spying
on personal communications. It is in this context that we present the
idea of a completely decentralised mode of video conferencing built over a peer-to-peer network, whose infrastructure is supported entirely by
its users, and is secured using advanced cryptography. A purely peer to
peer version of video communication would allow for communication to
take place directly between the parties involved. Identities can be taken
care of by using digital signatures. Distributed data structures hold the
necessary information required to maintain the network and to form a communication channel between two nodes. Since the infrastructure is dis
tributed over a p2p network, there is no cost associated with a dedicated
server. The project uses concepts from multimedia encoding, peer-to-peer
networks, bittorrents(DHT protocol), VoIP, and distributed systems.\\

Peer to peer network model, a significant networking model after client-server model, has generated considerable academic interests due to its many properties. This project implements a fully functional peer to peer network built using TCP/IP, and provided excellent learning oppertunities to study the subtleties and challenges of creating one such network.\\

The project as a stand alone application serves as a secure and decentralised communication tool, and may find application in fields where reliability, privacy and non interference are required.

	\end{abstract}
	
	\newpage
	\tableofcontents
	\newpage
	\listoffigures
	\newpage
	\printacronyms[include-classes=abbrev,name=Abbreviations]
	\newpage

	\pagenumbering{arabic}
	\section{Introduction}
        \subsection{Motivation and Overview}
        In the light of recent news regarding government and corporate spying, and blatant disregard for personal data, a secure application for unmonitored and unlogged communication seemed to be an apparent necessity. With principles of freedom and privacy of the individual as primary objectives, the system was designed to ensure security, lack of central authority, reliability and privacy. A peer to peer network matched the requirements with its properties like decetralised architecture, and reliability. The system posed several challenges and subtleties during implemenation which are of academic interest.\\

        The project is a decentralised video calling platform supported by a peer to peer network
much like a torrent network. Node machines run a software that connects the node to
the network. Pype uses asymmetric key cryptographic techniques to provide anonymity
and security. A decentralised network is resilient and harder to take down making it
more secure than a centralised client server based model. The system is also impartial as every node is identical and no node has more authority.

\subsection{Background and Literature Survey}
The poject is an original creation inspired from several successful peer to peer applications such as Bittorrent, bitcoin, and TOR Networks. Bittorrents are a peer to peer file sharing system. The DHT protocol used in Bittorrent inspired the AddrBook data structure used in this system. The Nakamoto consensus used in bitcoins for verifying authenticity of bitcoin blockchains is partially implemented to maintain consistency of dynamic data structures distributed over the network.\\

A significant obstacle to creating a peer to peer connection is the presence of NATs(Network Address Translators). NATs allow multiple applications and systems to operate using a single public IP address. A NAT allocates local IP addresses to devices connected to the NAT. Connections from these devices to the internet goes through, and its local IP address and port, along with its destination address and port is saved in a table with corresponding reassigned port. When a packet returns to device, the packet's source ip address, destination ip address and port are compared to that in the table and corresponding local ip address is reassigned to packet. When an IP packet from a source not on the table comes to a NAT, it is thrown away. It works well with the client-server network model, however causes issues with a peer to peer model since both peers could potentially be behind NATs. This issue is addressed using a scheme known as hole punching or NAT traversal. To open a NAT, a support server is used. When a peer requires a connection to an other peer, a request is sent to a server. The server is assumed to be connected to the other peer, and sends it a request. The recipient peer tries to establish a connection with the requesting peer. Since the requesting peer is already trying to establish connection, NAT is open for the requesting peer. When the other peer tries to establish a connection, its NAT is also opened and a connection is formed.

	\section{Design}
        \subsection{Operations}
        Once the application begins running, it tries to establish a connection with the support server to obtain its public IP address. It then requests a first peer to enter the network. Once connection is established to the first peer, it connects to more peers on the network. The user is presented with a window with options to Add Contact, generate new keys, select current keys, copy current keys and Make calls to user. If a call is made, it checks the address book entry corresponding to the callee hash address and decrypts the signature o discover callee node. A call request is sent to the callee node and awaits request accept. If request is accepted, the user is presented with a calling screen that contains video of user and video feed from the callee node, and an option to disconnect.
			\subsubsection{Design}
				To perform well in the long-term, we need to take into account not only the immediate rewards, but also the future rewards we are going to get. How should we go about that?

				Given one run of the Markov decision process, we can easily calculate the total reward for one episode:\\
				\begin{centering}
				\includegraphics[width=5cm]{../Design/images/dfr.png}\\
				\end{centering}
				Given that, the total future reward from time point t onward can be expressed as:\\
				\begin{centering}
				\includegraphics[width=7cm]{../Design/images/dfr2.png}\\
				\end{centering}
				But because our environment is stochastic, we can never be sure, if we will get the same rewards the next time we perform the same actions. The more into the future we go, the more it may diverge. For that reason it is common to use discounted future reward instead:\\
				\begin{centering}
				\includegraphics[width=9cm]{../Design/images/dfr3.png}\\
				\end{centering}
				Here Y is the discount factor between 0 and 1 the more into the future the reward is, the less we take it into consideration. It is easy to see, that discounted future reward at time step t can be expressed in terms of the same thing at time step t+1:\\
				\begin{centering}
				\includegraphics[width=10cm]{../Design/images/dfr4.png}\\
				\end{centering}
				If we set the discount factor Y=0, then our strategy will be short sighted and we rely only on the immediate rewards. If we want to balance between immediate and future rewards, we should set discount factor to something like Y=0.9. If our environment is deterministic and the same actions always result in same rewards, then we can set discount factor Y=1.\\

				A good strategy for an agent would be to always choose an action that maximizes the (discounted) future reward.
			\subsubsection{Q-Learning}
				In Q-learning we define a function Q(s, a) representing the maximum discounted future reward when we perform action a in state s, and continue optimally from that point on.

				\begin{centering}
				\includegraphics[width=5cm]{../Design/images/q.png}\\
				\end{centering}
				The way to think about Q(s, a) is that it is the best possible score at the end of the game after performing action a in state s. It is called Qfunction, because it represents the quality of a certain action in a given state.

				This may sound like quite a puzzling definition. How can we estimate the score at the end of game, if we know just the current state and action, and not the actions and rewards coming after that? We really cant. But as a theoretical construct we can assume existence of such a function. Just close your eyes and repeat to yourself five times: Q(s, a) exists, Q(s, a) exists, . Feel it?

				If youre still not convinced, then consider what the implications of having such a function would be. Suppose you are in state and pondering whether you should take action a or b. You want to select the action that results in the highest score at the end of game. Once you have the magical Q-function, the answer becomes really simple pick the action with the highest Q value!

				\begin{centering}
				\includegraphics[width=5cm]{../Design/images/q2.png}\\
				\end{centering}

				The above policy, the rule how we choose an action in each state.
				OK, how do we get that Q function then? Lets focus on just one transition <s, a, r, s>. Just like with discounted future rewards in the previous section, we can express the Q value of state s and action a in terms of the Q value of the next state s.

				\begin{centering}
				\includegraphics[width=5cm]{../Design/images/q3.png}\\
				\end{centering}

				This is called the Bellman equation. If you think about it, it is quite logical maximum future reward for this state and action is the immediate reward plus maximum future reward for the next state.
				The main idea in Q-learning is that we can iteratively approximate the Q-function using the Bellman equation. In the simplest case the Q-function is implemented as a table, with states as rows and actions as columns. The gist of the Q learning algorithm is as simple as the following:

				\begin{centering}
				\includegraphics[width=10cm]{../Design/images/q4.png}\\
				\end{centering}
			\subsubsection{Q Learning to Deep Q Network}
				Although a simple game with small number of states like tic tac toe can be easily implemented using a Q table data structure, the same approach does not work for an Atari game.

				The state of the environment in the Breakout game can be defined by the location of the paddle, location and direction of the ball and the presence or absence of each individual brick. But for something more universal, the obvious choice is screen pixels. They implicitly contain all of the relevant information about the game situation, except for the speed and direction of the ball. Two consecutive screens would have these covered as well.\\

				Taking our input of 84x84x4 pixels, In greyscale each pixel can have 128 values (atari could only emulate 128 colours), This would mean that the number of states is 128$^{84x84x4}$ which is more than the no of stars in the universe. Hence representing it in a Q table is out of the question.

				Since there are a lot of states, repetitions of states are very rare, Q table does poorly on states that it has never encountered before, While a neural network will give us an approximate value for an unknown state based on its current learning. 

				The time complexity of both Q table and Neural network remain the same at O(1) after completion of training, Although neural network might be a bit slower on account of the forward propagation it has to complete.


				Thus a shift from Q table approach to Deep Q network is done for the reasons
				\begin{itemize}
				\item Space complexity is containable even when number of states is very large
				\item Better approximation in un-encountered states
				\item Faster training with lesser data
				\end{itemize}
			\subsubsection{Deep Q-Learning}
				\mparagraph{Convolution Neural Networks}
					Neural networks are exceptionally good at coming up with good features for highly structured data. We could represent our Q function with a neural network, that takes the state (four game screens) and action as input and outputs the corresponding Q value. Alternatively we could take only game screens as input and output the Q-value for each possible action. This approach has the advantage, that if we want to perform a Q value update or pick the action with the highest Q value, we only have to do one forward pass through the network and have all Q values for all actions available immediately.

					Considering the fact that our input is an image input, we use a convolutional neural network. A convolutional network neuron functions similar to an optic neuron in the human brain. Each neuron instead of looking over the whole image, looks over only a small neighbourhood, and the combined effect of all the neurons performs the expected function.

					\begin{figure}[!ht]
						\begin{centering}
							\includegraphics[width=10cm]{../Design/images/deep.png}\\
							\caption{Two Possible models for Network.}
						\end{centering}
					\end{figure}

					\begin{figure}[!ht]
						\begin{centering}
							\includegraphics[width=10cm]{../Design/images/deep2.png}\\
							\caption{Network Design.}
						\end{centering}
					\end{figure}

					\begin{figure}[!ht]
						\begin{centering}
							\includegraphics[width=10cm]{../Design/images/deep4.png}\\
							\caption{Layer Representation}
						\end{centering}
					\end{figure}
		\subsection{Atari 2600}
		The Atari 2600 (or Atari VCS before 1982) is a home video game console released on September 11, 1977, by Atari, Inc. It is credited with popularizing the use of microprocessor-based hardware and ROM cartridges containing game code, a format first used with the Fairchild Channel F video game console in 1976. This format contrasts with the older model of having non-microprocessor dedicated hardware, which could only play the games that were physically built into the unit.

		The console was originally sold as the Atari VCS, an abbreviation for Video Computer System. Following the release of the Atari 5200 in 1982, the VCS was renamed to the ``Atari 2600'', after the unit's Atari part number, CX2600. The 2600 was typically bundled with two joystick controllers, a conjoined pair of paddle controllers, and a game cartridge: initially Combat, and later Pac-Man.
		\subsubsection{Controls}
		The Atari 2600 comes with a joystick controller consisting of a d-pad joystick and a fire button. The joystick can move in 8 different direction and the fire button can be pressed with or without joystick movement, those along with no-move present the 18 valid controls available in Atari.

		\subsubsection{Screen}
		Atari emulator consists of a 160x192 pixel screen, (though mostly it is converted into a square for use), The small input size can be further reduced by converting the screen to greyscale and decreasing image resolution to 84x84 pixels, Transient data is included by including multiple screens (4). 

		Thus the atari games are perfect for deep learning due to their small input size (84x84x4) and small output size (18) which works well with a deep neural network.
		
		\newpage
		\subsection{Program Development}
			\subsubsection{Class Diagram}
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=15cm]{../uml/uml.1}\\
						\caption{UML Class Diagram.}
					\end{centering}
				\end{figure}

		\subsection{Code Overview}
			The code for the project is open on github. https://github.com/devilsangel/deepQLearning

			\subsubsection{Environment}
				The environment is an interface that is used to control the environment of the game. It gives the actions that can be peformed at any given time, gets the current state of the system, act a particular action on the system and return the reward if there is one. It also alerts the agent when a terminal state is reached.

				The ALEEnvironment is the class that implements the environment interface. The structure was made so that different environments could be used interchangably with minimal change to the code. Other environments like gym environment can be used to implement the interface as well.

			\subsubsection{Agent}
				Agent class is the crux of the system. Which has variables of all other class types. In train mode, the system trains based on the qvalues provided by the deepQNetwork occasionally correcting it when rewards approach. After every step of the game, a callback to statisctics on step function is called. During training the loss of a life or the end of the game is treated as terminal states. Exploration rate controls whether exploration or exploitation is chosen. 

				In test mode, a given no of games are played, here the statebuffer is used to disallow contamination of trained data. Here only the end of the game is taken as a terminal state.

				In random mode, random actions are given to the system and the output observed.

			\subsubsection{Plotter}
				The plotter class uses python libraries to plot the data saved about each epoch as a graph. Visualising the important data metrics like average score and qvalue of each epoch, which can be used to evaluate the performance of the training.

				It also serves the function of plotting the maximum qvalue of the current screen live during play.
			
			\subsubsection{Statistics}
				The statistics class embeds itself as a callback in the agent class. The agent calls the on step method after every step of the game. If the game reaches a terminal state, the values like average score, no of games etc are updated in the statistics class.

				When the epoch ends, the write method is called which appends the calculated data to the csv file.

			\subsubsection{DeepQNetwork}
				An abstraction layer over Neon, a deep learning library from Nervana Systems. It Models the Network, finds the Q values associated with a state of the system, and does updation of weights based on reward and a random minibatch. The reward is used to refine the discounted future reward of the minibatch.

			\subsubsection{Replay Memory}
				For training the states, actions, screens and rewards need to be saved. These can be added to the replay memory using the add method. When a reward is obtained, a minibuffer is selected with the current state at the index position. This minibuffer is used for training the system.

			\subsubsection{State Buffer}
				During testing and play, the replay memory is not necessary, so a stripped down version of it, storing only the states of the system is used. This has the added advantage of ensuring that the data from testing does not pollute the training data.

	\section{Results and Discussions}
		The results of the training sessions were saved in a csv file in increments of 1 epoch or 250000 steps. Important parameters of the epoch like steps, number of games, average reward, mean cost, average q value etc are saved for three phases, training, test and random.


		\subsection{Description of observed strategies}
			\mparagraph{Breakout}
				Before training the system plays randomly, \emph{ie} chooses an action at random. 

				After one epoch the system adopts a simple strategy, to move to the left of the screen and stay there, an approach that assures a minimum score of two before defeat.

				After ten epochs the system plays like a human player, moving the paddle in response to the ball and dropping it occasionally.

				At fifty epochs, the play is significantly better occasionally resorting to the top strategy of tunnelling to the top of the screen from the edge.

			\mparagraph{Space Invaders}
				Space Invaders was trained for 8 epochs to prove that the algorithm can be used to train and play multiple games without any alterations proving the general purposse nature of it. At 8 epochs the game is played as a human would hitting the targets accurately enough and a consistent score in the range of 400 to 800 is obtained by the system.

		\subsection{Result Visualization}
			The results stored in csv files were plotted as a graph for visualization using matplotlib python module. Training of atari breakout was done for 50 epochs (65 hours). It  adopts visibly better strategies on later epochs. The training of space invaders was done for 8 epochs (10 hours), gameplay after 8 epochs was akin to a human player without any alteration to the algorithm, thus proving the general purpose nature of the algorithm.

			\begin{figure}[!h]
				\begin{centering}
					\includegraphics[width=15cm]{images/breakout.png}
					\caption{Breakout results for 50 epochs.}
				\end{centering}
			\end{figure}			
			
			\begin{figure}[!h]
				\begin{centering}
					\includegraphics[width=15cm]{images/space_invaders.png}
					\caption{Space Invaders results for 8 epochs.}
				\end{centering}
			\end{figure}			

	\section{Further Work}
		\subsection{Gamification of Problem}
			Solving real world problems can be achieved using Deep Reinforcement Learning. The following points are to be considered to convert a real world problem into a deep learning optimisation problem:
			\begin{itemize}
				\setlength\itemsep{0em}
				\item The problem is to be modelled as a game.
				\item The game should include an environment which has a finite set of actions.
				\item High dimensional input to the environment should be available as input and savable as a state.
				\item State of the system should change according to the actions performed on it.
				\item The environment should occasionally give off rewards in response to its current state or action performed.
			\end{itemize}

			If all of the criterions are met and a gamified version of the real world problem is made, then it can be quite easily given to a deep reinforcement learning system for optimisation and then later used in real life after training.

		\subsection{Traffic Light Control}
				A sample real world problem that could be solved using reinforced learning.
			\subsubsection{Problem}
				There is an intersection of two roads, and four traffic lights control the traffic in it, each controlling one road. The control of the traffic light should be given to an \ac{ai} for optimum traffic flow through the intersection. To design a system that can be used to train this \ac{ai}. 

			\subsubsection{Minimal Input}
				Traffic camera images at four different directions are taken as input to the system, mulitple images are taken to incorporate movement of the vehicles. Even though the system could work with the raw images, it would be impossible to train the system in the real world. So a simulation is required to train it. But in a simulation output images cannot be generated as complex as real video stream. 

				The solution is to simplify the input to the system, and to simulate the output in this simplified version. The approach taken is to convert the vehicles into white rectangles in a black canvas, and the intersection as a line. Thus white rectangles passing over the line will be taken for rewards and any intersection of rectangles will be interpreted as a crash. The rectangles are easier to simulate given the actions to be performed.

				To make the simulation closer to reality, delay between the signal turning green and the first car moving can be given. Contraction and elongation of traffic at stop and start can also be simulated.

				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic1.png}
						\caption{Empty Intersection.}
					\end{centering}
				\end{figure}

				
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic2.png}
						\caption{Intersection with Vehicles.}
					\end{centering}
				\end{figure}

				
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic3.png}
						\caption{Generation of Simplified Input.}
					\end{centering}
				\end{figure}

				
				\begin{figure}[!h]
					\begin{centering}
						\includegraphics[width=10cm]{images/traffic4.png}
						\caption{Simplified Input.}
					\end{centering}
				\end{figure}



			\subsubsection{Action set}
				At any instant, any of the traffic lights can be any of the following:
				\begin{itemize}
					\setlength\itemsep{0em}
					\item Red
					\item Orange
					\item Green in the Right, Left or Straight direction or any combination of it.
				\end{itemize}

				Orange can be eliminated by making it mandatory for transitions between Green and Red. Green has seven states (not to be confused with state of the system found from the input) it can be in for all the combinations. Since only Green or Red can be active at a given time, the total no of states for a single traffic light is 8.

				The intersection consists of 4 traffic lights, operating independantly (No constraints to its operation) making the total no of actions the system can take to $8^4$ \emph{ie} 4096 actions.

				Thus at every state of the system, it can decide between 4096 actions to take.
			\subsubsection{Reward}
				The no of vehicles that pass through the intersection in a time interval or the throughput of the intersection can be taken as a rudimentary reward function.

				To avoid any traffic signals that might incur collisions, any collisions detected are penalised with a negative reward. This still presents the problem of starvation \emph{ie.} a low amount of vehicles could wait for green indefinitely. It can be solved by aging the throughput negatively, which makes the throughput reward smaller and finally negative the longer it waits.
			\subsubsection{Scalability}
				The system proposed could be implemented on a larger scale, like a town or city. But in the current form of neural networks, adding a single traffic light to an existing system makes retraining the entire system a requirement. As the number of intersections increase, the number of actions rise exponentially. Thus making their efficient implementation a problem.

				The bottleneck is the non scalability of the current architecture, which could be tentatively solved if parallel neural networks are introduced. Parallel neural networks are still in its infancy and outside the scope of this project.
	\section{Conclusion}
		It is clear from the results that the presented model can used in applications with high dimensional sensory input. They also show that the algorithm will eventually converge to an optimal policy. Real world problems can be modelled as an environment with reward and actions, and plugged into the algorithm.

		Lack of processing power was observed as a pertinent limitation. The deepLearning library, Neon is optimized for Nvidia Maxwell Graphic Processing Units and its performance is very low on CPUs and pre-maxwell GPUs. We experimented training on GeForce GT 735M, Jetson TX1 board, and an i3 desktop CPU, the best among these was the GT 735M which took 77 mins per epoch.


	\newpage
	\nocite{*}
	\bibliography{helpers/bibliography}
	\bibliographystyle{ieeetr}

\end{document}